{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of v01 LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanjain1397/zip-the-code-1.0/blob/master/v_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7Pwq9fDJu2sG",
        "colab_type": "code",
        "outputId": "efb82fad-dcd0-433b-aabd-e71563fe3419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CmLSEOyhu7oS",
        "colab_type": "code",
        "outputId": "ed53083e-afb0-4b39-d90e-844446f81651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "tTl_vxGPvplJ",
        "colab_type": "code",
        "outputId": "67df0847-e338-41db-8d4f-24b14f8d2f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd drive/DL/Keras - Text"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/DL/Keras - Text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sVa2BcBlvqCU",
        "colab_type": "code",
        "outputId": "38fb0d54-351e-411a-bb4b-6faf29de34eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "import keras\n",
        "\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cvx0EdUFvsqf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \n",
        "    #Lowering case the text\n",
        "    text = text.lower()\n",
        "    \n",
        "    #Removing numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    \n",
        "    #Removing punctuations\n",
        "    import string\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    #Removing stop words\n",
        "    \n",
        "    tokens = word_tokenize(text)\n",
        "    text = ' '.join([ i for i in tokens if not i in stopwords])\n",
        "    \n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eoq0CcnvMV_G",
        "colab_type": "code",
        "outputId": "ffc8f45c-4a52-4321-cfb0-73dae0f94b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('./data/train_file.csv')\n",
        "test = pd.read_csv('./data/test_file.csv')\n",
        "sample = pd.read_csv('./data/sample_submission.csv')\n",
        "\n",
        "data.loc[data.Subjects.isnull(), 'Subjects'] = ''\n",
        "test.loc[test.Subjects.isnull(), 'Subjects'] = ''\n",
        "\n",
        "data['combined'] = data.Subjects.str.cat(' ' + data.Title)\n",
        "test['combined'] = test.Subjects.str.cat(' ' + test.Title)\n",
        "\n",
        "dataMatType = data.MaterialType\n",
        "data.drop(labels = ['MaterialType'], axis = 1, inplace = True)\n",
        "\n",
        "data.combined = data.combined.apply(preprocess_text)\n",
        "test.combined = test.combined.apply(preprocess_text)\n",
        "'''\n",
        "data.Subjects = data.Subjects.apply(preprocess_text)\n",
        "data.Title = data.Title.apply(preprocess_text)\n",
        "test.Subjects = test.Subjects.apply(preprocess_text)\n",
        "test.Title = test.Title.apply(preprocess_text)\n",
        "'''\n",
        "flag = -1\n",
        "labelDict = {}\n",
        "\n",
        "for key in dataMatType.value_counts().keys():\n",
        "  flag += 1\n",
        "  labelDict[key] = flag\n",
        "  \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 10                             \n",
        "\n",
        "max_words = 1000                                    \n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(data.combined)\n",
        "\n",
        "dataSequences = tokenizer.texts_to_sequences(data.combined)\n",
        "testSequences = tokenizer.texts_to_sequences(test.combined)\n",
        "\n",
        "data_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(data_word_index))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 36088 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AmgGkE7lvwc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import preprocessing\n",
        "x_train = preprocessing.sequence.pad_sequences(dataSequences, maxlen = 10)\n",
        "x_test = preprocessing.sequence.pad_sequences(testSequences, maxlen = 10)\n",
        "\n",
        "train_label = dataMatType.map(labelDict)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "labelDataBinary = to_categorical(train_label)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(dataMatType), dataMatType)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PFlbzeBoGMX9",
        "colab_type": "code",
        "outputId": "4939380c-9475-4ef0-ce05-17503f4d005d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "glove_dir = './glove.6B'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in data_word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "a1af66d0-3193-4146-db31-ba67cc269cbd",
        "id": "p8lV4j7qvUNd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000, 300, input_length=10))\n",
        "\n",
        "'''model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "'''\n",
        "\n",
        "\n",
        "model.add(Conv1D(128, 3,  activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(LSTM(400, recurrent_dropout = 0.6, return_sequences = True))\n",
        "model.add(Dropout(0.6))\n",
        "\n",
        "model.add(LSTM(400, recurrent_dropout = 0.2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, labelDataBinary,\n",
        "                    epochs=8,\n",
        "                    validation_split=0.2,\n",
        "                    class_weight = class_weights)\n",
        "\n",
        "model.save('./models/m5.h5')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25322 samples, validate on 6331 samples\n",
            "Epoch 1/8\n",
            "25322/25322 [==============================] - 58s 2ms/step - loss: 0.5827 - acc: 0.8373 - val_loss: 0.4636 - val_acc: 0.8727\n",
            "Epoch 2/8\n",
            "25322/25322 [==============================] - 50s 2ms/step - loss: 0.4632 - acc: 0.8698 - val_loss: 0.4404 - val_acc: 0.8785\n",
            "Epoch 3/8\n",
            "25322/25322 [==============================] - 50s 2ms/step - loss: 0.4425 - acc: 0.8743 - val_loss: 0.4657 - val_acc: 0.8789\n",
            "Epoch 4/8\n",
            "25322/25322 [==============================] - 49s 2ms/step - loss: 0.4271 - acc: 0.8776 - val_loss: 0.4289 - val_acc: 0.8807\n",
            "Epoch 5/8\n",
            "25322/25322 [==============================] - 49s 2ms/step - loss: 0.4190 - acc: 0.8805 - val_loss: 0.4479 - val_acc: 0.8782\n",
            "Epoch 6/8\n",
            "25322/25322 [==============================] - 49s 2ms/step - loss: 0.4146 - acc: 0.8804 - val_loss: 0.4816 - val_acc: 0.8837\n",
            "Epoch 7/8\n",
            "25322/25322 [==============================] - 49s 2ms/step - loss: 0.4115 - acc: 0.8822 - val_loss: 0.4838 - val_acc: 0.8833\n",
            "Epoch 8/8\n",
            "25322/25322 [==============================] - 49s 2ms/step - loss: 0.4020 - acc: 0.8838 - val_loss: 0.4494 - val_acc: 0.8820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K-vJg1_cvU9W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('./models/m1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a8EGozWZ3UdC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Using Embedding Layer**"
      ]
    },
    {
      "metadata": {
        "id": "CNdQiMIGwjQD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000, 300, input_length=10, weights=[embedding_matrix], trainable=False))\n",
        "\n",
        "'''model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "'''\n",
        "\n",
        "model.add(LSTM(1000, recurrent_dropout = 0.3))\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, labelDataBinary,\n",
        "                    epochs=8,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2,\n",
        "                    class_weight = class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZReVSR3gy5kI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reverseLabelDict = {}\n",
        "\n",
        "for key, value in labelDict.items():\n",
        "  reverseLabelDict[value] = key\n",
        "  \n",
        "predictions = model.predict(x_test)\n",
        "predictions = pd.Series([np.argmax(i) for i in predictions], index = test.index)\n",
        "predictions = predictions.map(reverseLabelDict)\n",
        "\n",
        "submission = pd.concat([test.ID.astype(np.int), predictions], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJCiVVuTQ-x6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission.to_csv('sub-mew.csv', index = False, header = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4azj14Q4A2R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(model, x_test):\n",
        "  predictions = model.predict(x_test)\n",
        "  predictions = pd.Series([np.argmax(i) for i in predictions], index = test.index)\n",
        "  predictions = predictions.map(reverseLabelDict)\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ie1SXk352-SI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_model = 5\n",
        "predic1 = pd.Series(predict(model1, x_test))\n",
        "predic2 = pd.Series(predict(model2, x_test))\n",
        "predic3 = pd.Series(predict(model3, x_test))\n",
        "predic4 = pd.Series(predict(model4, x_test))\n",
        "predic5 = pd.Series(predict(model5, x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RjMF0-A7FJZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total = pd.DataFrame(pd.concat([predic1, predic2, predic3, predic4, predic5], axis = 1))\n",
        "submissions = total.mode(axis = 1)[0]\n",
        "submission = pd.concat([test.ID.astype(np.int), predictions], axis = 1)\n",
        "submission.columns = ['ID' , 'MaterialType']\n",
        "submission.to_csv('sub-mew.csv', index = False, header = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LrPJl9KF8O40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1094
        },
        "outputId": "97116bc7-9e22-49c5-e434-bd56c460d593"
      },
      "cell_type": "code",
      "source": [
        "submissions = total.mode(axis = 1)[0]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0             BOOK\n",
              "1        VIDEOCASS\n",
              "2        SOUNDDISC\n",
              "3             BOOK\n",
              "4        VIDEOCASS\n",
              "5             BOOK\n",
              "6        SOUNDDISC\n",
              "7             BOOK\n",
              "8             BOOK\n",
              "9             BOOK\n",
              "10       VIDEOCASS\n",
              "11            BOOK\n",
              "12            BOOK\n",
              "13            BOOK\n",
              "14            BOOK\n",
              "15            BOOK\n",
              "16            BOOK\n",
              "17            BOOK\n",
              "18       SOUNDDISC\n",
              "19            BOOK\n",
              "20            BOOK\n",
              "21            BOOK\n",
              "22            BOOK\n",
              "23            BOOK\n",
              "24            BOOK\n",
              "25            BOOK\n",
              "26            BOOK\n",
              "27           MIXED\n",
              "28            BOOK\n",
              "29            BOOK\n",
              "           ...    \n",
              "21072    VIDEODISC\n",
              "21073         BOOK\n",
              "21074         BOOK\n",
              "21075         BOOK\n",
              "21076         BOOK\n",
              "21077         BOOK\n",
              "21078         BOOK\n",
              "21079    SOUNDDISC\n",
              "21080         BOOK\n",
              "21081         BOOK\n",
              "21082         BOOK\n",
              "21083         BOOK\n",
              "21084         BOOK\n",
              "21085         BOOK\n",
              "21086         BOOK\n",
              "21087         BOOK\n",
              "21088         BOOK\n",
              "21089         BOOK\n",
              "21090         BOOK\n",
              "21091         BOOK\n",
              "21092         BOOK\n",
              "21093         BOOK\n",
              "21094         BOOK\n",
              "21095         BOOK\n",
              "21096         BOOK\n",
              "21097         BOOK\n",
              "21098         BOOK\n",
              "21099         BOOK\n",
              "21100    VIDEOCASS\n",
              "21101    SOUNDDISC\n",
              "Name: 0, Length: 21102, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    }
  ]
}